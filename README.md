# SynLog+: Improving the Templates Identified by Syntax-based Log Parsers

## Datasets Characteristics

| Software systems          | # Annotated Logs (Loghub-2.0) | # Templates  (Loghub-2.0) | # Templates (Loghub-2k) |
| ------------------------- | ------------------------- | --------------------- | ----------------------- |
| Hadoop                    | 179,993                   | 236                   | 114                     |
| OpenStack                 | 207,632                   | 48                    | 43                      |
| Zookeeper                 | 74,273                    | 89                    | 50                      |
| HPC                       | 429,987                   | 74                    | 46                      |
| Linux                     | 23,921                    | 338                   | 118                     |
| Mac                       | 100,314                   | 626                   | 341                     |
| Apache                    | 51,977                    | 29                    | 6                       |
| OpenSSH                   | 638,946                   | 38                    | 27                      |
| HealthApp                 | 212,394                   | 156                   | 75                      |
| Proxifier                 | 21,320                    | 11                    | 8                       |


## Repository Organization

```
├── 2k_dataset/ # The original Loghub-2k datasets
├── full_dataset/ # Loghub-2.0 datasets
├── benchmark/
│   ├── evaluation/ # Evaluation scripts for benchmark log parsers
│   ├── logparser/  # Benchmark log parsers (syntax-based)
│   ├── old_benchmark/
│   ├── LogPPT/     # contains the modified source code of LogPPT
│   ├── LLMParser/  # contains the modified source code of LLMParser
│   └── UniParser/  # contains the source code of implemented UniParser
├── result/
│   ├── ...... #
│   └── ...... # contains the output evaluation metric files and all parsed results
├── requirements.txt
└── README.MD
```

## Requirements

### System

Owing to the large scale of the benchmark datasets in the experiments, the
requirements of the benchmark of all log parsers are:

- At least 16GB memory
- At least 100GB storage
- GPU (for LogPPT, UniParser, and LLMParser)

### Dependencies

1. Python 3.10
2. Packages listed in requirements.txt


## Run evaluation

To evaluate a log parser, the following steps are taken:

1. The logs in the datasets are parsed by the log parser
2. The parsed logs are evaluated for accuracy

### Running the syntax-based log parsers

The 9 benchmark syntax-based log parsers can be run with their respective runner
scripts in the `benchmark/evaluation/` directory. Example commands are provided
below.  Replace `Drain` with other syntax-based log parsers.

```
pushd benchmark/evaluation/
python Drain_run.py             # For Loghub-2k datasets
python Drain_run.py -full       # For Loghub-2.0 datasets
```

### Running SynLog+

Replace `Drain` with other syntax-based log parsers for it to be considered as
the grouping module.

```
pushd benchmark/evaluation/
python SynLogPlus_run.py -g Drain -full
```

### Running semantic-based log parsers

Since these techniques are different from other syntax-based parsers and also
from each other, we seperate their environments from other log parsers.  Please
refer to the individual README files for UniParser, LogPPT, and LLMParser.


### Evaluating the log parsing results

When we have the parsed logs of the log parsers, we can evaluate the accuracy
across 4 accuracy metrics GA, PA, FGA, and FTA with the help of our evaluator
script. The runner scripts store the parsed logs in the `results/` directory.
The evaluator script accepts the directory path of the parsed logs, as shown in
the command below.  The evaluator scripts prints out the 4 accuracy metrics for
each dataset along with the average accuracies.  The script also stores the
evaluation results on a CSV file `results.csv` inside the directory provied.

```
pushd benchmark/
python evaluator.py --dirpath ../result/result_Drain_full/ --use_full
```

### Preparing the log parsing results

You can combine the results generated by the `evaluator.py` script with the
`prepresults.py` script in 'benchmark/' directory.  It creates an excel sheet
with the four accuracy scores obtained by the benchmark log parsers along with
SynLog+.  For each dataset and for the average scores, the highest accuracy will
be highlighted with bold font.

```
pushd prepresults.py -full
```
